{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda environment (project)\n",
    "# /home/student/.conda/envs/project/bin/python \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os, gc\n",
    "import torch\n",
    "\n",
    "from transformers import set_seed\n",
    "from datasets import load_dataset\n",
    "from evaluate import *\n",
    "from arc.arc import ARCSolver\n",
    "\n",
    "from datasets import Dataset\n",
    "from utils import render_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 300 different tasks in the dataset.\n",
      "# of samples in a task: min(82), Q1(1000), Q2(1000), Q3(1000), max(1000), mean(897.7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>train</th>\n",
       "      <th>test_input</th>\n",
       "      <th>test_output</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>239be575</td>\n",
       "      <td>[{'input': [[0, 0, 6, 6, 2, 0, 0], [2, 0, 6, 6...</td>\n",
       "      <td>[{'input': [[0, 5, 5, 3, 3, 3, 0], [0, 5, 5, 0...</td>\n",
       "      <td>[[[0]]]</td>\n",
       "      <td>[{'input': [[0, 5, 5, 3, 3, 3, 0], [0, 5, 5, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4258a5f9</td>\n",
       "      <td>[{'input': [[3, 3, 3, 3], [3, 3, 3, 3], [3, 3,...</td>\n",
       "      <td>[{'input': [[6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 2...</td>\n",
       "      <td>[[[6, 6, 6, 1, 1, 1], [1, 1, 1, 1, 2, 1], [1, ...</td>\n",
       "      <td>[{'input': [[6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1caeab9d</td>\n",
       "      <td>[{'input': [[6, 6, 6, 6, 6, 6, 6], [6, 6, 8, 8...</td>\n",
       "      <td>[{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4...</td>\n",
       "      <td>[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0,...</td>\n",
       "      <td>[{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>623ea044</td>\n",
       "      <td>[{'input': [[3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3...</td>\n",
       "      <td>[{'input': [[6, 6, 6, 6, 6], [6, 6, 6, 6, 6], ...</td>\n",
       "      <td>[[[6, 8, 6, 6, 6], [6, 6, 8, 6, 8], [6, 6, 6, ...</td>\n",
       "      <td>[{'input': [[6, 6, 6, 6, 6], [6, 6, 6, 6, 6], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e9afcf9a</td>\n",
       "      <td>[{'input': [[4, 4, 4, 4, 4], [6, 6, 6, 6, 6], ...</td>\n",
       "      <td>[{'input': [[5, 5, 5, 5, 5, 5, 5, 5, 5], [7, 7...</td>\n",
       "      <td>[[[5, 7, 5, 7, 5, 7, 5, 7, 5], [7, 0, 7, 0, 7,...</td>\n",
       "      <td>[{'input': [[5, 5, 5, 5, 5, 5, 5, 5, 5], [7, 7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       task                                              train  \\\n",
       "0  239be575  [{'input': [[0, 0, 6, 6, 2, 0, 0], [2, 0, 6, 6...   \n",
       "1  4258a5f9  [{'input': [[3, 3, 3, 3], [3, 3, 3, 3], [3, 3,...   \n",
       "2  1caeab9d  [{'input': [[6, 6, 6, 6, 6, 6, 6], [6, 6, 8, 8...   \n",
       "3  623ea044  [{'input': [[3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3...   \n",
       "4  e9afcf9a  [{'input': [[4, 4, 4, 4, 4], [6, 6, 6, 6, 6], ...   \n",
       "\n",
       "                                          test_input  \\\n",
       "0  [{'input': [[0, 5, 5, 3, 3, 3, 0], [0, 5, 5, 0...   \n",
       "1  [{'input': [[6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 2...   \n",
       "2  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4...   \n",
       "3  [{'input': [[6, 6, 6, 6, 6], [6, 6, 6, 6, 6], ...   \n",
       "4  [{'input': [[5, 5, 5, 5, 5, 5, 5, 5, 5], [7, 7...   \n",
       "\n",
       "                                         test_output  \\\n",
       "0                                            [[[0]]]   \n",
       "1  [[[6, 6, 6, 1, 1, 1], [1, 1, 1, 1, 2, 1], [1, ...   \n",
       "2  [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0,...   \n",
       "3  [[[6, 8, 6, 6, 6], [6, 6, 8, 6, 8], [6, 6, 6, ...   \n",
       "4  [[[5, 7, 5, 7, 5, 7, 5, 7, 5], [7, 0, 7, 0, 7,...   \n",
       "\n",
       "                                                test  \n",
       "0  [{'input': [[0, 5, 5, 3, 3, 3, 0], [0, 5, 5, 0...  \n",
       "1  [{'input': [[6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 2...  \n",
       "2  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4...  \n",
       "3  [{'input': [[6, 6, 6, 6, 6], [6, 6, 6, 6, 6], ...  \n",
       "4  [{'input': [[5, 5, 5, 5, 5, 5, 5, 5, 5], [7, 7...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the test dataset\n",
    "data_path = \"dataset\"\n",
    "dataset, task_list = load_data(data_path)\n",
    "df300 = sample_data(dataset, task_list, n_row=30000) \n",
    "df300.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare samples for each task\n",
    "task_samples = []\n",
    "for t in range(300):\n",
    "    df = sample_data(dataset, task_list, n_row=1000, indices=[t])\n",
    "    task_samples.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "==================================================\n",
      "Example input\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #008080\">  </span><span style=\"background-color: #00ff00\">  </span><span style=\"background-color: #800080\">  </span><span style=\"background-color: #008000\">    </span><span style=\"background-color: #800080\">  </span><span style=\"background-color: #00ff00\">  </span><span style=\"background-color: #008080\">  </span>  [6, 9, 5, 2, 2, 5, 9, 6]\n",
       "<span style=\"background-color: #00ff00\">  </span><span style=\"background-color: #ff0000\">  </span><span style=\"background-color: #800080\">  </span><span style=\"background-color: #800000\">    </span><span style=\"background-color: #800080\">  </span><span style=\"background-color: #ff0000\">  </span><span style=\"background-color: #00ff00\">  </span>  [9, 8, 5, 1, 1, 5, 8, 9]\n",
       "<span style=\"background-color: #800080\">      </span><span style=\"background-color: #008000\">    </span><span style=\"background-color: #800080\">      </span>  [5, 5, 5, 2, 2, 5, 5, 5]\n",
       "<span style=\"background-color: #808000\">      </span><span style=\"background-color: #008080\">    </span><span style=\"background-color: #008000\">  </span><span style=\"background-color: #800000\">  </span><span style=\"background-color: #008000\">  </span>  [3, 3, 3, 6, 6, 2, 1, 2]\n",
       "<span style=\"background-color: #008000\">  </span><span style=\"background-color: #800000\">  </span><span style=\"background-color: #008000\">  </span><span style=\"background-color: #008080\">    </span><span style=\"background-color: #008000\">  </span><span style=\"background-color: #800000\">  </span><span style=\"background-color: #008000\">  </span>  [2, 1, 2, 6, 6, 2, 1, 2]\n",
       "<span style=\"background-color: #800080\">      </span><span style=\"background-color: #008000\">    </span><span style=\"background-color: #800080\">      </span>  [5, 5, 5, 2, 2, 5, 5, 5]\n",
       "<span style=\"background-color: #00ff00\">  </span><span style=\"background-color: #ff0000\">  </span><span style=\"background-color: #800080\">  </span><span style=\"background-color: #800000\">    </span><span style=\"background-color: #800080\">  </span><span style=\"background-color: #ff0000\">  </span><span style=\"background-color: #00ff00\">  </span>  [9, 8, 5, 1, 1, 5, 8, 9]\n",
       "<span style=\"background-color: #008080\">  </span><span style=\"background-color: #00ff00\">  </span><span style=\"background-color: #800080\">  </span><span style=\"background-color: #008000\">    </span><span style=\"background-color: #800080\">  </span><span style=\"background-color: #00ff00\">  </span><span style=\"background-color: #008080\">  </span>  [6, 9, 5, 2, 2, 5, 9, 6]\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[46m  \u001b[0m\u001b[102m  \u001b[0m\u001b[45m  \u001b[0m\u001b[42m  \u001b[0m\u001b[42m  \u001b[0m\u001b[45m  \u001b[0m\u001b[102m  \u001b[0m\u001b[46m  \u001b[0m  [6, 9, 5, 2, 2, 5, 9, 6]\n",
       "\u001b[102m  \u001b[0m\u001b[101m  \u001b[0m\u001b[45m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[45m  \u001b[0m\u001b[101m  \u001b[0m\u001b[102m  \u001b[0m  [9, 8, 5, 1, 1, 5, 8, 9]\n",
       "\u001b[45m  \u001b[0m\u001b[45m  \u001b[0m\u001b[45m  \u001b[0m\u001b[42m  \u001b[0m\u001b[42m  \u001b[0m\u001b[45m  \u001b[0m\u001b[45m  \u001b[0m\u001b[45m  \u001b[0m  [5, 5, 5, 2, 2, 5, 5, 5]\n",
       "\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[46m  \u001b[0m\u001b[46m  \u001b[0m\u001b[42m  \u001b[0m\u001b[41m  \u001b[0m\u001b[42m  \u001b[0m  [3, 3, 3, 6, 6, 2, 1, 2]\n",
       "\u001b[42m  \u001b[0m\u001b[41m  \u001b[0m\u001b[42m  \u001b[0m\u001b[46m  \u001b[0m\u001b[46m  \u001b[0m\u001b[42m  \u001b[0m\u001b[41m  \u001b[0m\u001b[42m  \u001b[0m  [2, 1, 2, 6, 6, 2, 1, 2]\n",
       "\u001b[45m  \u001b[0m\u001b[45m  \u001b[0m\u001b[45m  \u001b[0m\u001b[42m  \u001b[0m\u001b[42m  \u001b[0m\u001b[45m  \u001b[0m\u001b[45m  \u001b[0m\u001b[45m  \u001b[0m  [5, 5, 5, 2, 2, 5, 5, 5]\n",
       "\u001b[102m  \u001b[0m\u001b[101m  \u001b[0m\u001b[45m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[45m  \u001b[0m\u001b[101m  \u001b[0m\u001b[102m  \u001b[0m  [9, 8, 5, 1, 1, 5, 8, 9]\n",
       "\u001b[46m  \u001b[0m\u001b[102m  \u001b[0m\u001b[45m  \u001b[0m\u001b[42m  \u001b[0m\u001b[42m  \u001b[0m\u001b[45m  \u001b[0m\u001b[102m  \u001b[0m\u001b[46m  \u001b[0m  [6, 9, 5, 2, 2, 5, 9, 6]\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #008000\">  </span><span style=\"background-color: #800000\">  </span><span style=\"background-color: #008000\">  </span>  [2, 1, 2]\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[42m  \u001b[0m\u001b[41m  \u001b[0m\u001b[42m  \u001b[0m  [2, 1, 2]\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Example test input\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #800000\">                    </span>  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
       "<span style=\"background-color: #800000\">                    </span>  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
       "<span style=\"background-color: #800000\">            </span><span style=\"background-color: #808000\">      </span><span style=\"background-color: #800000\">  </span>  [1, 1, 1, 1, 1, 1, 3, 3, 3, 1]\n",
       "<span style=\"background-color: #800000\">            </span><span style=\"background-color: #808000\">      </span><span style=\"background-color: #800000\">  </span>  [1, 1, 1, 1, 1, 1, 3, 3, 3, 1]\n",
       "<span style=\"background-color: #800000\">            </span><span style=\"background-color: #808000\">      </span><span style=\"background-color: #800000\">  </span>  [1, 1, 1, 1, 1, 1, 3, 3, 3, 1]\n",
       "<span style=\"background-color: #800000\">            </span><span style=\"background-color: #808000\">      </span><span style=\"background-color: #800000\">  </span>  [1, 1, 1, 1, 1, 1, 3, 3, 3, 1]\n",
       "<span style=\"background-color: #800000\">            </span><span style=\"background-color: #808000\">      </span><span style=\"background-color: #800000\">  </span>  [1, 1, 1, 1, 1, 1, 3, 3, 3, 1]\n",
       "<span style=\"background-color: #800000\">            </span><span style=\"background-color: #808000\">      </span><span style=\"background-color: #800000\">  </span>  [1, 1, 1, 1, 1, 1, 3, 3, 3, 1]\n",
       "<span style=\"background-color: #800000\">            </span><span style=\"background-color: #808000\">      </span><span style=\"background-color: #800000\">  </span>  [1, 1, 1, 1, 1, 1, 3, 3, 3, 1]\n",
       "<span style=\"background-color: #800000\">                    </span>  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1, 1, 1, 1, 3, 3, 3, 1]\n",
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1, 1, 1, 1, 3, 3, 3, 1]\n",
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1, 1, 1, 1, 3, 3, 3, 1]\n",
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1, 1, 1, 1, 3, 3, 3, 1]\n",
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1, 1, 1, 1, 3, 3, 3, 1]\n",
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1, 1, 1, 1, 3, 3, 3, 1]\n",
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[43m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1, 1, 1, 1, 3, 3, 3, 1]\n",
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example test output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #800000\">      </span>  [1, 1, 1]\n",
       "<span style=\"background-color: #800000\">      </span>  [1, 1, 1]\n",
       "<span style=\"background-color: #800000\">      </span>  [1, 1, 1]\n",
       "<span style=\"background-color: #800000\">      </span>  [1, 1, 1]\n",
       "<span style=\"background-color: #800000\">      </span>  [1, 1, 1]\n",
       "<span style=\"background-color: #800000\">      </span>  [1, 1, 1]\n",
       "<span style=\"background-color: #800000\">      </span>  [1, 1, 1]\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1]\n",
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1]\n",
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1]\n",
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1]\n",
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1]\n",
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1]\n",
       "\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m\u001b[41m  \u001b[0m  [1, 1, 1]\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Visualize a task (EDA)\n",
    "task_indices = [17] # select which task you want to examine\n",
    "n_sample = 1\n",
    "for task_idx in task_indices:\n",
    "    print(task_idx)\n",
    "    for data in Dataset.from_pandas(task_samples[task_idx]).shuffle().select(range(n_sample)):\n",
    "        for case in data['train']:\n",
    "            print(\"==================================================\")\n",
    "            print(\"Example input\")\n",
    "            render_grid(case['input'])\n",
    "            print(\"Example output\")\n",
    "            render_grid(case['output'])\n",
    "            break\n",
    "        print(\"==================================================\")\n",
    "        print(\"Example test input\")\n",
    "        render_grid(data['test'][0]['input'])\n",
    "        print(\"Example test output\")\n",
    "        render_grid(data['test'][0]['output'])\n",
    "    print(\"==================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 3, 7, 8, 9, 10, 11, 13, 15, 18, 19, 20, 22, 27, 29, 30, 32, 33, 34, 35, 38, 40, 41, 44, 45, 46, 51, 52, 54, 55, 56, 57, 58, 61, 64, 65, 66, 67, 68, 69, 70, 72, 73, 75, 78, 79, 80, 81, 83, 85, 87, 89, 90, 91, 92, 93, 95, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 110, 111, 113, 114, 116, 117, 118, 119, 120, 123, 124, 128, 129, 130, 133, 135, 137, 138, 139, 140, 141, 142, 143, 145, 147, 148, 150, 151, 153, 154, 155, 157, 158, 159, 160, 161, 162, 166, 167, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 185, 188, 190, 193, 194, 195, 196, 197, 198, 201, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 218, 220, 222, 224, 225, 226, 227, 228, 230, 231, 233, 234, 235, 236, 239, 244, 245, 246, 248, 249, 250, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 267, 268, 270, 271, 273, 274, 275, 276, 277, 281, 282, 285, 286, 287, 291, 292, 293, 295, 296, 297]\n"
     ]
    }
   ],
   "source": [
    "simple_tasks = []\n",
    "hard_tasks = []\n",
    "for task_idx in range(300):\n",
    "    check = True\n",
    "    for data in Dataset.from_pandas(task_samples[task_idx]).shuffle().select(range(3)):\n",
    "        for case in data['train']:\n",
    "            wi, hi = len(case['input'][0]), len(case['input'])\n",
    "            wo, ho = len(case['output'][0]), len(case['output'])\n",
    "            if (wi!=wo) or (hi!=ho): check = False\n",
    "        case = data['test'][0]\n",
    "        wi, hi = len(case['input'][0]), len(case['input'])\n",
    "        wo, ho = len(case['output'][0]), len(case['output'])\n",
    "        if (wi!=wo) or (hi!=ho): check = False\n",
    "    if check: simple_tasks.append(task_idx)\n",
    "    else: hard_tasks.append(task_idx)\n",
    "print(simple_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1234567890)\n",
    "token = os.environ.get(\"HF_TOKEN\", None)\n",
    "solver = ARCSolver(model_id=\"Qwen/Qwen3-1.7B\", hf_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# solver.prepare_train()\n",
    "n_train = len(hard_tasks)*1000\n",
    "n_eval = 500\n",
    "dfsimple = sample_data(dataset, task_list, n_row=n_train+n_eval, indices=simple_tasks, random=56)\n",
    "dfhard = sample_data(dataset, task_list, n_row=n_train+n_eval, indices=hard_tasks, random=56)\n",
    "train_dataset = Dataset.from_pandas(dfsimple).select(range(n_train))\n",
    "# solver.train(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcbe3f824fd44eab026a85ae954e68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_eval = 100\n",
    "solver.prepare_evaluation(select_adapter=\"20250702_220556\") # make sure you set the right model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cf6ad3e75741fc916fa54aca5ea71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[tensor([151644,   8948,    198,   2610,    525,    458,   6203,    369,  21828,\n",
      "         19819,  46523,     13,   1446,    525,   2661,   1045,  13530,    315,\n",
      "          1946,    323,   2550,    438,    220,     17,     35,   5827,    448,\n",
      "           279,   1852,  16533,  27979,   5383,   1948,   1105,     13,   1446,\n",
      "           614,    311,  73045,  23583,    279,   4734,   5912,    429,  28475,\n",
      "          1817,   1946,    311,    279,  12159,   2550,    624, 151645,    198,\n",
      "        151644,    872,    198,   8420,    525,    279,   3110,   1946,    323,\n",
      "          2550,  13530,    504,    892,    498,   1265,   3960,    279,  16533,\n",
      "          5912,    311,   2937,   7023,    279,   2550,    369,    279,   2661,\n",
      "          1273,   1946,    510,   1408,  28665,   1355,    510,     17,     20,\n",
      "            20,     20,     20,     20,    198,     20,     20,     20,     20,\n",
      "            23,     20,    198,     20,     20,     20,     20,     20,     23,\n",
      "           198,     20,     20,     20,     20,     20,     20,    198,     20,\n",
      "            20,     20,     20,     20,     20,    198,     20,     23,     20,\n",
      "            20,     20,     20,    198,     20,     20,     20,     24,     20,\n",
      "            20,    198,     20,     21,     20,     20,     20,     20,    198,\n",
      "            20,     20,     20,     20,     20,     20,    198,   3006,    510,\n",
      "            17,     16,     17,     16,     17,     16,    198,     20,     20,\n",
      "            20,     20,     23,     16,    198,     20,     20,     20,     20,\n",
      "            20,     23,    198,     20,     20,     20,     20,     20,     20,\n",
      "           198,     20,     20,     20,     20,     20,     20,    198,     20,\n",
      "            23,     16,     23,     16,     23,    198,     20,     20,     20,\n",
      "            24,     16,     24,    198,     20,     21,     16,     21,     16,\n",
      "            21,    198,     20,     20,     20,     20,     20,     20,    198,\n",
      "           198,   1355,    510,     22,     22,     22,     22,     22,     22,\n",
      "            22,     22,     20,     22,    198,     22,     22,     22,     22,\n",
      "            22,     22,     22,     22,     22,     22,    198,     22,     20,\n",
      "            22,     22,     22,     22,     22,     22,     22,     22,    198,\n",
      "          3006,    510,     22,     22,     22,     22,     22,     22,     22,\n",
      "            22,     20,     16,    198,     22,     22,     22,     22,     22,\n",
      "            22,     22,     22,     22,     22,    198,     22,     20,     16,\n",
      "            20,     16,     20,     16,     20,     16,     20,    198,    198,\n",
      "          1355,    510,     19,     19,     19,     19,     19,     19,     20,\n",
      "            19,    198,     19,     19,     19,     19,     19,     19,     19,\n",
      "            19,    198,     19,     19,     19,     19,     19,     19,     19,\n",
      "            19,    198,     19,     19,     19,     19,     19,     19,     19,\n",
      "            19,    198,     19,     19,     19,     19,     19,     19,     19,\n",
      "            19,    198,     19,     19,     19,     19,     19,     19,     19,\n",
      "            20,    198,     19,     19,     19,     19,     19,     19,     19,\n",
      "            20,    198,   3006,    510,     19,     19,     19,     19,     19,\n",
      "            19,     20,     16,    198,     19,     19,     19,     19,     19,\n",
      "            19,     19,     19,    198,     19,     19,     19,     19,     19,\n",
      "            19,     19,     19,    198,     19,     19,     19,     19,     19,\n",
      "            19,     19,     19,    198,     19,     19,     19,     19,     19,\n",
      "            19,     19,     19,    198,     19,     19,     19,     19,     19,\n",
      "            19,     19,     20,    198,     19,     19,     19,     19,     19,\n",
      "            19,     19,     20,    198,    198,    198,   1408,  28665,   7039,\n",
      "            11,  11625,    279,   2701,  24626,   3118,    389,   1181,   1946,\n",
      "          5827,    553,  18950,    279,   5601,    498,    614,   9498,    504,\n",
      "           279,   4862,    821,     13,    510,   1408,  28665,   1355,    510,\n",
      "            18,     24,     18,    198,     18,     24,     18,    198,     18,\n",
      "            21,     18,    198,     18,     21,     18,    198,    198,   1408,\n",
      "         28665,   3838,    374,    279,   2550,   5827,     30,   8278,   3410,\n",
      "           279,   2550,   5827,    304,    279,   1352,    438,    304,    279,\n",
      "          3110,   1946,    323,   2550,  13530,     13,   3155,    537,   3410,\n",
      "           894,   5107,   1995,    510, 151645,    198, 151644,  77091,    198,\n",
      "          3006,    510,     16,     24,     16,    198,     16,     24,     16,\n",
      "           198,     16,     21,     16,    198,     16,     21,     16,    198,\n",
      "        151645])]\n",
      "[[5 6 5]\n",
      " [5 6 5]\n",
      " [5 2 5]\n",
      " [5 2 5]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #800080\">  </span><span style=\"background-color: #008080\">  </span><span style=\"background-color: #800080\">  </span>  [5 6 5]\n",
       "<span style=\"background-color: #800080\">  </span><span style=\"background-color: #008080\">  </span><span style=\"background-color: #800080\">  </span>  [5 6 5]\n",
       "<span style=\"background-color: #800080\">  </span><span style=\"background-color: #008000\">  </span><span style=\"background-color: #800080\">  </span>  [5 2 5]\n",
       "<span style=\"background-color: #800080\">  </span><span style=\"background-color: #008000\">  </span><span style=\"background-color: #800080\">  </span>  [5 2 5]\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[45m  \u001b[0m\u001b[46m  \u001b[0m\u001b[45m  \u001b[0m  [5 6 5]\n",
       "\u001b[45m  \u001b[0m\u001b[46m  \u001b[0m\u001b[45m  \u001b[0m  [5 6 5]\n",
       "\u001b[45m  \u001b[0m\u001b[42m  \u001b[0m\u001b[45m  \u001b[0m  [5 2 5]\n",
       "\u001b[45m  \u001b[0m\u001b[42m  \u001b[0m\u001b[45m  \u001b[0m  [5 2 5]\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      6\u001b[39m eval_dataset = Dataset.from_pandas(task_samples[task]).select(\u001b[38;5;28mrange\u001b[39m(n_eval))\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m eval_data \u001b[38;5;129;01min\u001b[39;00m tqdm(eval_dataset):\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# print(\"============================================\")\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# print(\"Test input\")\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# render_grid(eval_data[\"test\"][0]['input'])\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# print(\"Predict output\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     preds = \u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_bfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# list of grids\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m preds:\n\u001b[32m     15\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: render_grid(p)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:19\u001b[39m, in \u001b[36mpredict_bfs\u001b[39m\u001b[34m(self, datapoint)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/peft/peft_model.py:1757\u001b[39m, in \u001b[36mPeftModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1755\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   1756\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1757\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1758\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1759\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1760\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1761\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1762\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1763\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1764\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1765\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1766\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1769\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1770\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:193\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:730\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    725\u001b[39m output_hidden_states = (\n\u001b[32m    726\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    727\u001b[39m )\n\u001b[32m    729\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    743\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    744\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:463\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    461\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/transformers/modeling_layers.py:48\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:284\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:248\u001b[39m, in \u001b[36mQwen3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    235\u001b[39m attn_output, attn_weights = attention_interface(\n\u001b[32m    236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    237\u001b[39m     query_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m     **kwargs,\n\u001b[32m    245\u001b[39m )\n\u001b[32m    247\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m attn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:494\u001b[39m, in \u001b[36mLinear4bit.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    492\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.base_layer(x, *args, **kwargs)\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m     \u001b[38;5;66;03m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[39;00m\n\u001b[32m    496\u001b[39m     \u001b[38;5;66;03m# The reason is that in some cases, an error can occur that backprop\u001b[39;00m\n\u001b[32m    497\u001b[39m     \u001b[38;5;66;03m# does not work on a manipulated view. This issue may be solved with\u001b[39;00m\n\u001b[32m    498\u001b[39m     \u001b[38;5;66;03m# newer PyTorch versions but this would need extensive testing to be\u001b[39;00m\n\u001b[32m    499\u001b[39m     \u001b[38;5;66;03m# sure.\u001b[39;00m\n\u001b[32m    500\u001b[39m     result = result.clone()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:484\u001b[39m, in \u001b[36mLinear4bit.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    480\u001b[39m     x = x.to(\u001b[38;5;28mself\u001b[39m.compute_dtype)\n\u001b[32m    482\u001b[39m bias = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias.to(\u001b[38;5;28mself\u001b[39m.compute_dtype)\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m.to(inp_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:533\u001b[39m, in \u001b[36mmatmul_4bit\u001b[39m\u001b[34m(A, B, quant_state, out, bias)\u001b[39m\n\u001b[32m    531\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:462\u001b[39m, in \u001b[36mMatMul4Bit.forward\u001b[39m\u001b[34m(ctx, A, B, out, bias, quant_state)\u001b[39m\n\u001b[32m    458\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.empty(A.shape[:-\u001b[32m1\u001b[39m] + B_shape[:\u001b[32m1\u001b[39m], dtype=A.dtype, device=A.device)\n\u001b[32m    460\u001b[39m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[32m    461\u001b[39m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m output = torch.nn.functional.linear(A, \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m.to(A.dtype).t(), bias)\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[32m    465\u001b[39m ctx.state = quant_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.arcproj/lib/python3.12/site-packages/bitsandbytes/functional.py:1359\u001b[39m, in \u001b[36mdequantize_4bit\u001b[39m\u001b[34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[39m\n\u001b[32m   1356\u001b[39m         absmax = absmax.float()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1359\u001b[39m     out = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mA\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1361\u001b[39m n = out.numel()\n\u001b[32m   1363\u001b[39m is_on_gpu([A, absmax, out])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# evaluate our model (eval set)\n",
    "scores = []\n",
    "n_eval = 20\n",
    "scores_task = []\n",
    "for task in range(20):\n",
    "    eval_dataset = Dataset.from_pandas(task_samples[task]).select(range(n_eval))\n",
    "    for eval_data in tqdm(eval_dataset):\n",
    "        # print(\"============================================\")\n",
    "        # print(\"Test input\")\n",
    "        # render_grid(eval_data[\"test\"][0]['input'])\n",
    "\n",
    "        # print(\"Predict output\")\n",
    "        preds = solver.predict_bfs(eval_data) # list of grids\n",
    "        for p in preds:\n",
    "            if p is not None: render_grid(p)\n",
    "        # pred = solver.predict(eval_data) # single grid\n",
    "        # if pred is not None: render_grid(pred)\n",
    "\n",
    "        # print(\"Test output\")\n",
    "        # render_grid(eval_data[\"test\"][0]['output'])\n",
    "        # print(\"============================================\")\n",
    "\n",
    "        s = 0\n",
    "        for p in preds:\n",
    "            if p is None: s += 0\n",
    "            else: s += check_match(p, eval_data[\"test\"][0][\"output\"]) \n",
    "        # s = check_match(pred, eval_data[\"test\"][0][\"output\"])\n",
    "        scores.append(s) # s = 0 or 1\n",
    "    score = np.array(scores).mean() * 100\n",
    "    scores_task.append(score)\n",
    "    print(f\"Evaluation score: {score:.2f}\", flush=True)\n",
    "    scores = []\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "x = np.arange(20)\n",
    "plt.bar(x, scores_task)\n",
    "plt.xticks(x, list(range(20)))\n",
    "plt.ylim(0,100)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".arcproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
