{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda environment (project)\n",
    "# /home/student/.conda/envs/project/bin/python \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os, gc\n",
    "import torch\n",
    "\n",
    "from transformers import set_seed\n",
    "from datasets import load_dataset\n",
    "from evaluate import *\n",
    "from arc.arc import ARCSolver\n",
    "\n",
    "from datasets import Dataset\n",
    "from utils import render_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the test dataset\n",
    "data_path = \"dataset\"\n",
    "dataset, task_list = load_data(data_path)\n",
    "df300 = sample_data(dataset, task_list, n_row=30000, random=112) \n",
    "df300.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare samples for each task\n",
    "task_samples = []\n",
    "for t in range(300):\n",
    "    df = sample_data(dataset, task_list, n_row=1000, indices=[t])\n",
    "    task_samples.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize a task (EDA)\n",
    "task_indices = [1, 6, 8, 10, 13, 15, 17] # select which task you want to examine\n",
    "n_sample = 1\n",
    "for task_idx in task_indices:\n",
    "    print(task_idx)\n",
    "    for data in Dataset.from_pandas(task_samples[task_idx]).shuffle().select(range(n_sample)):\n",
    "        for case in data['train']:\n",
    "            print(\"==================================================\")\n",
    "            print(\"Example input\")\n",
    "            render_grid(case['input'])\n",
    "            print(\"Example output\")\n",
    "            render_grid(case['output'])\n",
    "            break\n",
    "        print(\"==================================================\")\n",
    "        print(\"Example test input\")\n",
    "        render_grid(data['test'][0]['input'])\n",
    "        print(\"Example test output\")\n",
    "        render_grid(data['test'][0]['output'])\n",
    "    print(\"==================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tasks = []\n",
    "hard_tasks = []\n",
    "for task_idx in range(300):\n",
    "    check = True\n",
    "    for data in Dataset.from_pandas(task_samples[task_idx]).shuffle().select(range(3)):\n",
    "        for case in data['train']:\n",
    "            wi, hi = len(case['input'][0]), len(case['input'])\n",
    "            wo, ho = len(case['output'][0]), len(case['output'])\n",
    "            if (wi!=wo) or (hi!=ho): check = False\n",
    "        case = data['test'][0]\n",
    "        wi, hi = len(case['input'][0]), len(case['input'])\n",
    "        wo, ho = len(case['output'][0]), len(case['output'])\n",
    "        if (wi!=wo) or (hi!=ho): check = False\n",
    "    if check: simple_tasks.append(task_idx)\n",
    "    else: hard_tasks.append(task_idx)\n",
    "print(simple_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1234567890)\n",
    "token = os.environ.get(\"HF_TOKEN\", None)\n",
    "solver = ARCSolver(model_id=\"Qwen/Qwen3-1.7B\", hf_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# solver.prepare_train()\n",
    "n_train = 30000\n",
    "n_eval = 500\n",
    "dfsimple = sample_data(dataset, task_list, n_row=n_train+n_eval, indices=simple_tasks, random=56)\n",
    "dfhard = sample_data(dataset, task_list, n_row=n_train+n_eval, indices=hard_tasks, random=56)\n",
    "train_dataset = Dataset.from_pandas(df300).select(range(n_train))\n",
    "# solver.train(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval = 100\n",
    "solver.prepare_evaluation(select_adapter=\"20250711_170015\") # make sure you set the right model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our model (eval set)\n",
    "scores = []\n",
    "n_eval = 20\n",
    "task_indices = [1,6,8,9,10,13,14,15,17] # list(range(20))\n",
    "scores_task = []\n",
    "for task in task_indices:\n",
    "    eval_dataset = Dataset.from_pandas(task_samples[task]).select(range(n_eval))\n",
    "    for eval_data in tqdm(eval_dataset):\n",
    "        # print(\"============================================\")\n",
    "        # print(\"Test input\")\n",
    "        # render_grid(eval_data[\"test\"][0]['input'])\n",
    "\n",
    "        # print(\"Predict output\")\n",
    "        # solver.train_testtime(eval_data) # TTT\n",
    "        preds = [ solver.predict(eval_data, use_ttt=False) for _ in range(10) ] # augmented sampling\n",
    "        # for p in preds:\n",
    "        #     if p is not None: render_grid(p)\n",
    "        # if pred is not None: render_grid(pred)\n",
    "\n",
    "        # print(\"Test output\")\n",
    "        # render_grid(eval_data[\"test\"][0]['output'])\n",
    "        # print(\"============================================\")\n",
    "        s = 0\n",
    "        for p in preds:\n",
    "            if p is None: continue\n",
    "            else: s = max(s, check_match(p, eval_data[\"test\"][0][\"output\"]))\n",
    "        # if pred is None: s = 0\n",
    "        # else: s = check_match(pred, eval_data[\"test\"][0][\"output\"])\n",
    "        scores.append(s)\n",
    "    score = np.array(scores).mean() * 100\n",
    "    scores_task.append(score)\n",
    "    print(f\"Evaluation score: {score:.2f}\", flush=True)\n",
    "    scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "x = np.arange(9)\n",
    "plt.bar(x, scores_task)\n",
    "plt.xticks(x, task_indices)\n",
    "plt.ylim(0,100)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".arcproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
