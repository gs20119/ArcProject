{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda environment (project)\n",
    "# /home/student/.conda/envs/project/bin/python \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os, gc\n",
    "import torch\n",
    "\n",
    "from transformers import set_seed\n",
    "from datasets import load_dataset\n",
    "from evaluate import *\n",
    "from arc import ARCSolver\n",
    "\n",
    "from datasets import Dataset\n",
    "from utils import render_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# prepare the test dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m dataset, task_list \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m sample_data(dataset, task_list, n_row\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m) \n\u001b[1;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m) \n",
      "File \u001b[0;32m/workspace/arcproj_jw/evaluate.py:26\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(base_dir)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(base_dir):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m########################################################################\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# load the dataset (json)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# each element of the dataset is a list of cases in a single task\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# each sample is in a form of dictionary\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m########################################################################\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     filenames \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     data_files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m filenames \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m p]\n\u001b[1;32m     28\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset'"
     ]
    }
   ],
   "source": [
    "# prepare the test dataset\n",
    "data_path = \"dataset\"\n",
    "dataset, task_list = load_data(data_path)\n",
    "df = sample_data(dataset, task_list, n_row=10000) \n",
    "df.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare samples for each task\n",
    "task_samples = []\n",
    "for t in range(300):\n",
    "    df = sample_data(dataset, task_list, n_row=1000, indices=[t])\n",
    "    task_samples.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a task (EDA)\n",
    "task_idx = 39 # select which task you want to examine\n",
    "n_sample = 1\n",
    "for data in Dataset.from_pandas(task_samples[task_idx]).shuffle().select(range(n_sample)):\n",
    "    for case in data['train']:\n",
    "        print(\"==================================================\")\n",
    "        print(\"Example input\")\n",
    "        render_grid(case['input'])\n",
    "        print(\"Example output\")\n",
    "        render_grid(case['output'])\n",
    "    print(\"==================================================\")\n",
    "    print(\"Example test input\")\n",
    "    render_grid(data['test'][0]['input'])\n",
    "    print(\"Example test output\")\n",
    "    render_grid(data['test'][0]['output'])\n",
    "print(\"==================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our model(arcsolver) instance\n",
    "set_seed(1234567890)\n",
    "token = os.environ.get(\"HF_TOKEN\", None)\n",
    "solver = ARCSolver(token=token) # default: finetuning-sample (given sample adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.prepare_train()\n",
    "n_train = 500\n",
    "train_dataset = Dataset.from_pandas(task_samples[task_idx]).shuffle(42).select(range(n_train))\n",
    "solver.train(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval = 10\n",
    "eval_dataset = Dataset.from_pandas(task_samples[task_idx]).shuffle(42).select(range(n_train,n_train+n_eval))\n",
    "solver.prepare_evaluation(select_adapter=\"20250527_100116\") # make sure you set the right model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our model (eval set)\n",
    "scores = []\n",
    "for eval_data in tqdm(eval_dataset):\n",
    "    print(\"============================================\")\n",
    "    print(\"Test input\")\n",
    "    render_grid(eval_data[\"test\"][0]['input'])\n",
    "\n",
    "    print(\"Predict output\")\n",
    "    preds = solver.predict(eval_data)\n",
    "    render_grid(preds)\n",
    "\n",
    "    print(\"Test output\")\n",
    "    render_grid(eval_data[\"test\"][0]['output'])\n",
    "    print(\"============================================\")\n",
    "    if preds is None: s = 0\n",
    "    else: s = check_match(preds, eval_data[\"test\"][0][\"output\"])\n",
    "    scores.append(s)\n",
    "\n",
    "score = np.array(scores).mean() * 100\n",
    "print(f\"Evaluation scores: {score:.2f}\", flush=True)\n",
    "print(\"Evaluation Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.prepare_train()\n",
    "n_train = 10000\n",
    "df20 = sample_data(dataset, task_list, n_row=10500, indices=list(range(20)))\n",
    "train_dataset = Dataset.from_pandas(df20).select(range(n_train))\n",
    "solver.train(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval = 20\n",
    "solver.prepare_evaluation(select_adapter=\"20250527_011623\") # make sure you set the right model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from safetensors import safe_open\n",
    "# from safetensors.torch import save_file\n",
    "\n",
    "# safetensors 파일 열기\n",
    "# with safe_open(\"artifacts/20250527_011623/checkpoint-final/adapter_model.safetensors\", framework=\"pt\", device=\"cpu\") as f:\n",
    "#     for k in f.keys(): print(k)\n",
    "#     state_dict = {k: f.get_tensor(k) for k in f.keys()}\n",
    "\n",
    "# fixed_state_dict = {}\n",
    "# for key in state_dict:\n",
    "#     new_key = key.replace(\"base_model.model.base_model.model.model\", \"base_model.model.model\")\n",
    "#     fixed_state_dict[new_key] = state_dict[key]\n",
    "\n",
    "# save_file(fixed_state_dict, \"artifacts/20250527_011623/checkpoint-final/adapter_model_fixed.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our model (eval set)\n",
    "scores = []\n",
    "scores_task = []\n",
    "for task in range(20,40):\n",
    "    eval_dataset = Dataset.from_pandas(task_samples[task]).shuffle(42).select(range(n_eval))\n",
    "    for eval_data in tqdm(eval_dataset):\n",
    "        # print(\"============================================\")\n",
    "        # print(\"Test input\")\n",
    "        # render_grid(eval_data[\"test\"][0]['input'])\n",
    "\n",
    "        # print(\"Predict output\")\n",
    "        preds = solver.predict(eval_data)\n",
    "        # render_grid(preds)\n",
    "\n",
    "        # print(\"Test output\")\n",
    "        # render_grid(eval_data[\"test\"][0]['output'])\n",
    "        # print(\"============================================\")\n",
    "        if preds is None: s = 0\n",
    "        else: s = check_match(preds, eval_data[\"test\"][0][\"output\"])\n",
    "        scores.append(s)\n",
    "    score = np.array(scores).mean() * 100\n",
    "    scores_task.append(score)\n",
    "    print(f\"Evaluation score: {score:.2f}\", flush=True)\n",
    "    scores = []\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "x = np.arange(20,40)\n",
    "plt.bar(x, scores_task)\n",
    "plt.xticks(x, list(range(20,40)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
